{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('http://www.baidu.com')\n",
    "r.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\r\\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=百度一下 class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write(\\'<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\\'+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ \\'\" name=\"tj_login\" class=\"lb\">登录</a>\\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\\r\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding\n",
    "r.apparent_encoding\n",
    "r.encoding = 'utf-8'\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://www.baidu.com')\n",
    "'''\n",
    "r.status_code = 200-访问成功 404或其他-访问失败\n",
    "r.text  获取页面内容，字符串\n",
    "r.encoding 从header获取编码方式\n",
    "r.apparent_encoding 从内容中获取编码方式（备选编码方式）\n",
    "r.content 二进制形式\n",
    "requests.request()构造一个请求，支撑以下各方法的基础方法 \n",
    "\n",
    "requests.get()获取HTML网页的主要方法，对应于HTTP的GET \n",
    "requests.head()【r.headers】获取HTML网页头信息的方法，对应于HTTP的HEAD \n",
    "\n",
    "requests.post()向HTML网页提交POST请求的方法，对应于HTTP的POST \n",
    "requests.put()向HTML网页提交PUT请求的方法，对应于HTTP的PUT \n",
    "requests.patch()向HTML网页提交局部修改请求，对应于HTTP的PATCH \n",
    "\n",
    "requests.delete()向HTML页面提交删除请求，对应于HTTP的DELETE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用下行语句判断是否存在异常\n",
    "r.raise_for_status()\n",
    "'''常见异常\n",
    "requests.ConnectionError网络连接错误异常，如DNS查询失败、拒绝连接等\n",
    "requests.HTTPErrorHTTP错误异常\n",
    "requests.URLRequiredURL缺失异常\n",
    "requests.TooManyRedirects超过最大重定向次数，产生重定向异常\n",
    "requests.ConnectTimeout连接远程服务器超时异常\n",
    "requests.Timeout请求URL超时，产生超时异常'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬100次的时间是9.702203512191772秒\n"
     ]
    }
   ],
   "source": [
    "#通用代码框架\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout = 30)\n",
    "        r.raise_for_status()#如果状态不是200，引发HTTPerror\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return '产生异常！'\n",
    "if __name__ == '__main__':\n",
    "    start_time=time.time()\n",
    "    for i in range(100):\n",
    "        getHTMLText('http://www.baidu.com')\n",
    "    end_time=time.time()\n",
    "    print(\"爬100次的时间是{}秒\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''request详细介绍\n",
    "1.requests.request(method,url,**kwargs)\n",
    "method : 请求方式，对应get/put/post等7种 \n",
    "【GET,HEAD,POST,PUT,PATCH,delete,OPTIONS】\n",
    "url     : 拟获取页面的url链接 \n",
    "**kwargs: 控制访问的参数，共13个\n",
    "【params: 字典或字节序列，作为参数增加到url中 \n",
    "**data  : 字典、字节序列或文件对象，作为Request的内容\n",
    "**json  : JSON格式的数据，作为Request的内容\n",
    "**headers : 字典，HTTP定制头\n",
    "  cookies : 字典或CookieJar，Request中的cookie\n",
    "  auth  : 元组，支持HTTP认证功能\n",
    "  files : 字典类型，传输文件\n",
    "  timeout : 设定超时时间，秒为单位\n",
    "  proxies : 字典类型，设定访问代理服务器，可以增加登 录认证\n",
    "  allow_redirects: True/False，默认为True，重定向开关\n",
    "  stream  : True/False，默认为True，获取内容立即下载开关\n",
    "  verify  : True/False，默认为True，认证SSL证书开关\n",
    "  cert   : 本地SSL证书路径】\n",
    "2.requests.head(url,**kwargs)\n",
    "3.requests.post(url,data=None, json=None, **kwargs)\n",
    "4.requests.put(url,data=None, **kwargs)\n",
    "5.requests.patch(url,data=None, **kwargs)\n",
    "6.requests.delete(url,**kwargs)\n",
    "7.requests.get(url,params=None, **kwargs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''robots 协议\n",
    "*代表所有，/代表根目录 \n",
    "User‐agent: *\n",
    "Disallow: / \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-19-b2579c338e9e>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-b2579c338e9e>\"\u001b[1;36m, line \u001b[1;32m45\u001b[0m\n\u001b[1;33m    root = 'D:''\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "实例1：京东商品页面的爬取 \n",
    "实例2：亚马逊商品页面的爬取 \n",
    "实例3：百度搜索关键字提交 \n",
    "实例4：网络图片的爬取和存储 \n",
    "实例5：IP地址归属地的自动查询\n",
    "'''\n",
    "#代码1\n",
    "import requests\n",
    "url = 'https://item.jd.com'\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[:1000])\n",
    "except:\n",
    "    print('失败')\n",
    "#代码2 \n",
    "import requests\n",
    "url = 'https://www.amazon.cn'\n",
    "try:\n",
    "    kv = {'user-agent':'Mozilla/5.0'}\n",
    "    r = requests.get(url,headers=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[1000:2000])\n",
    "except:\n",
    "    print('失败')\n",
    "#代码3\n",
    "import requests\n",
    "keyword = 'Python'\n",
    "url = 'https://item.jd.com'\n",
    "try:\n",
    "    kv = {'wd':keyword}\n",
    "    r = requests.get(url,params=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[:1000])\n",
    "except:\n",
    "    print('失败')\n",
    "#代码4\n",
    "import requests\n",
    "import os\n",
    "url = '...'\n",
    "root = 'D:'\n",
    "path = root + url.split('/')[-1]\n",
    "try:\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    if not os.path.exists(path):\n",
    "        r = requests.get(url)\n",
    "        with open (path,'wb') as f:\n",
    "            f.write(r.content)\n",
    "            f.close()\n",
    "            print('文件保存成功')\n",
    "    else:\n",
    "        print('文件已存在')\n",
    "except:\n",
    "    print('爬取失败')\n",
    "#代码5\n",
    "import requests\n",
    "url = 'http://m.ip138.com/ip.asp?ip='\n",
    "try:\n",
    "    r = requests.get(url+'202.204.80.112')\n",
    "    r.raise_for_statu()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[:1000])\n",
    "except:\n",
    "    print('失败')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   This is a python demo page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The demo python introduces several python courses.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"course\">\n",
      "   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "   <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">\n",
      "    Basic Python\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">\n",
      "    Advanced Python\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"解析器说明\\nbs4的HTML解析器BeautifulSoup(mk,'html.parser')安装bs4库 \\nlxml的HTML解析器BeautifulSoup(mk,'lxml')pip\\xa0install\\xa0lxml \\nlxml的XML解析器BeautifulSoup(mk,'xml')pip\\xa0install\\xa0lxml \\nhtml5lib的解析器BeautifulSoup(mk,'html5lib')pip\\xa0install\\xa0html5lib\\n基本元素\\nTag标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾 \\nName标签的名字，<p>…</p>的名字是'p'，格式：<tag>.name \\nAttributes标签的属性，字典形式组织，格式：<tag>.attrs \\nNavigableString标签内非属性字符串，<>…</>中字符串，格式：<tag>.string \\nComment标签内字符串的注释部分，一种特殊的Comment类型\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('http://python123.io/ws/demo.html')\n",
    "demo = r.text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(demo,'html.parser') \n",
    "\n",
    "print(soup.prettify())#格式输出\n",
    "'''解析器说明\n",
    "bs4的HTML解析器BeautifulSoup(mk,'html.parser')安装bs4库 \n",
    "lxml的HTML解析器BeautifulSoup(mk,'lxml')pip install lxml \n",
    "lxml的XML解析器BeautifulSoup(mk,'xml')pip install lxml \n",
    "html5lib的解析器BeautifulSoup(mk,'html5lib')pip install html5lib\n",
    "基本元素\n",
    "Tag标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾 \n",
    "Name标签的名字，<p>…</p>的名字是'p'，格式：<tag>.name \n",
    "Attributes标签的属性，字典形式组织，格式：<tag>.attrs \n",
    "NavigableString标签内非属性字符串，<>…</>中字符串，格式：<tag>.string \n",
    "Comment标签内字符串的注释部分，一种特殊的Comment类型\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''遍历方式：下行/上行/平行遍历\n",
    "1.下行遍历：\n",
    ".contents子节点的列表，将<tag>所有儿子节点存入列表 \n",
    ".children子节点的迭代类型，与.contents类似，用于循环遍历儿子节点 \n",
    ".descendants子孙节点的迭代类型，包含所有子孙节点，用于循环遍历\n",
    "for child in soup.body.children:\n",
    "    print(child)\n",
    "for child in soup.body.descendants:\n",
    "    print(child)\n",
    "\n",
    "2.上行遍历：\n",
    ".parent节点的父亲标签 \n",
    ".parents节点先辈标签的迭代类型，用于循环遍历先辈节点\n",
    "for parent in soup.a.parents:\n",
    "    if parent is none:\n",
    "        print(parent)\n",
    "    else:\n",
    "        print(parent.name)\n",
    "        \n",
    "3.平行遍历：\n",
    ".next_sibling返回按照HTML文本顺序的下一个平行节点标签 \n",
    ".previous_sibling返回按照HTML文本顺序的上一个平行节点标签 \n",
    ".next_siblings迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 \n",
    ".previous_siblings迭代类型，返回按照HTML文本顺序的前续所有平行节点标签\n",
    "for sibling in soup.a.next_sibling:\n",
    "    print(sibling)遍历后续节点\n",
    "for sibling in soup.a.previous_sibling:\n",
    "    print(sibling)遍历前续节点\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''三种信息标记形式的格式及其比较\n",
    "1.XML\n",
    "<name> … </name>\n",
    "<name />\n",
    "<!‐‐‐‐>\n",
    "2.JSON\n",
    "“key” :“value”\n",
    "“key” :[“value1”,“value2”]\n",
    "“key” : {“subkey” :“subvalue”}\n",
    "3.YAML\n",
    "key :value\n",
    "key :#Comment\n",
    "‐value1 \n",
    "‐value2\n",
    "key :  \n",
    "    subkey:subvalue \n",
    "\n",
    "XML   最早的通用信息标记语言，可扩展性好，但繁琐\n",
    "JSON  信息有类型，适合程序处理(js)，较XML简洁，无注释\n",
    "YAML  信息无类型，文本信息比例最高，可读性好，有注释易读\n",
    "\n",
    "内容查找\n",
    "<>.find_all(name,attrs,recursive, string, **kwargs)\n",
    "∙name : 对标签名称的检索字符串 \n",
    "∙attrs: 对标签属性值的检索字符串，可标注属性检索 \n",
    "∙recursive: 是否对子孙全部检索，默认True\n",
    "∙string: <>…</>中字符串区域的检索字符串\n",
    "\n",
    "<tag>(..) 等价于<tag>.find_all(..)\n",
    "soup(..) 等价于soup.find_all(..)\n",
    "\n",
    "拓展方法：\n",
    "<>.find()搜索且只返回一个结果，同.find_all()参数\n",
    "<>.find_parents()在先辈节点中搜索，返回列表类型，同.find_all()参数\n",
    "<>.find_parent()在先辈节点中返回一个结果，同.find()参数\n",
    "<>.find_next_siblings()在后续平行节点中搜索，返回列表类型，同.find_all()参数\n",
    "<>.find_next_sibling()在后续平行节点中返回一个结果，同.find()参数\n",
    "<>.find_previous_siblings()在前序平行节点中搜索，返回列表类型，同.find_all()参数\n",
    "<>.find_previous_sibling()在前序平行节点中返回一个结果，同.find()参数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'正则表达式（简洁表达一组字符串）\\n常用操作符\\n.      表示任何单个字符\\n[\\xa0]   字符集，对单个字符给出取值范围    [abc]表示a、b、c，[a‐z]表示a到z单个字符\\n[^\\xa0]  非字符集，对单个字符给出排除范围  [^abc]表示非a或b或c的单个字符\\n*      前一个字符0次或无限次扩展         abc*表示ab、abc、abcc、abccc等\\n+      前一个字符1次或无限次扩展         abc+表示abc、abcc、abccc等\\n?      前一个字符0次或1次扩展            abc?表示ab、abc\\n|      左右表达式任意一个                abc|def表示abc、def\\n{m}    扩展前一个字符m次                 ab{2}c表示abbc\\n{m,n}  扩展前一个字符m至n次（含n）       ab{1,2}c表示abc、abbc\\n^      匹配字符串开头                    ^abc表示abc且在一个字符串的开头\\n$      匹配字符串结尾                    abc$表示abc且在一个字符串的结尾\\n(\\xa0)   分组标记，内部只能使用|操作符     (abc)表示abc，(abc|def)表示abc、def\\n\\\\d     数字，等价于[0‐9]\\n\\\\w     单词字符，等价于[A‐Za‐z0‐9_]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''正则表达式（简洁表达一组字符串）\n",
    "常用操作符\n",
    ".      表示任何单个字符\n",
    "[ ]   字符集，对单个字符给出取值范围    [abc]表示a、b、c，[a‐z]表示a到z单个字符\n",
    "[^ ]  非字符集，对单个字符给出排除范围  [^abc]表示非a或b或c的单个字符\n",
    "*      前一个字符0次或无限次扩展         abc*表示ab、abc、abcc、abccc等\n",
    "+      前一个字符1次或无限次扩展         abc+表示abc、abcc、abccc等\n",
    "?      前一个字符0次或1次扩展            abc?表示ab、abc\n",
    "|      左右表达式任意一个                abc|def表示abc、def\n",
    "{m}    扩展前一个字符m次                 ab{2}c表示abbc\n",
    "{m,n}  扩展前一个字符m至n次（含n）       ab{1,2}c表示abc、abbc\n",
    "^      匹配字符串开头                    ^abc表示abc且在一个字符串的开头\n",
    "$      匹配字符串结尾                    abc$表示abc且在一个字符串的结尾\n",
    "( )   分组标记，内部只能使用|操作符     (abc)表示abc，(abc|def)表示abc、def\n",
    "\\d     数字，等价于[0‐9]\n",
    "\\w     单词字符，等价于[A‐Za‐z0‐9_]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n",
      "['100081', '100086']\n",
      "100081\n",
      "100086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bit:zipcode tsu:zipcode'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "match对象的属性\n",
    ".string 待匹配的文本\n",
    ".re     匹配时使用的patter对象（正则表达式）\n",
    ".pos    正则表达式搜索文本的开始位置\n",
    ".endpos 正则表达式搜索文本的结束位置\n",
    "match对象的方法\n",
    ".group(0) 获得匹配后的字符串\n",
    ".start()  匹配字符串在原始字符串的开始位置\n",
    ".end()    匹配字符串在原始字符串的结束位置\n",
    ".span()   返回(.start(), .end())\n",
    "\n",
    "re库主要函数\n",
    "re.search()   在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象\n",
    "re.match()    从一个字符串的开始位置起匹配正则表达式，返回match对象\n",
    "re.findall()  搜索字符串，以列表类型返回全部能匹配的子串\n",
    "re.split()    将一个字符串按照正则表达式匹配结果进行分割，返回列表类型\n",
    "re.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象\n",
    "re.sub()      在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串\n",
    "1.re.search(pattern,string,flags=0)\n",
    "∙ pattern : 正则表达式的字符串或原生字符串表示 \n",
    "∙ string  : 待匹配字符串 \n",
    "∙ flags   : 正则表达式使用时的控制标记\n",
    "    re.Ire.IGNORECASE忽略正则表达式的大小写，[A‐Z]能够匹配小写字符 \n",
    "    re.Mre.MULTILINE正则表达式中的^操作符能够将给定字符串的每行当作匹配开始 \n",
    "    re.Sre.DOTALL正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符\n",
    "2.re.match(pattern,string,flags=0)\n",
    "∙ pattern : 正则表达式的字符串或原生字符串表示\n",
    "∙ string  : 待匹配字符串 \n",
    "∙ flags   : 正则表达式使用时的控制标记\n",
    "3.re.findall(pattern,string,flags=0)\n",
    "∙ pattern : 正则表达式的字符串或原生字符串表示\n",
    "∙ string  : 待匹配字符串\n",
    "∙ flags   : 正则表达式使用时的控制标记\n",
    "4.re.split(pattern,string,maxsplit=0,flags=0)\n",
    "∙ pattern : 正则表达式的字符串或原生字符串表示\n",
    "∙ string  : 待匹配字符串\n",
    "∙ maxsplit : 最大分割数，剩余部分作为后一个元素输出\n",
    "∙ flags   : 正则表达式使用时的控制标记\n",
    "5.re.finditer(pattern,string,flags=0)\n",
    "∙ pattern : 正则表达式的字符串或原生字符串表示 \n",
    "∙ string  : 待匹配字符串 \n",
    "∙ flags   : 正则表达式使用时的控制标记\n",
    "6.re.sub(pattern,repl,string,count=0,flags=0)\n",
    "∙ pattern : 正则表达式的字符串或原生字符串表示\n",
    "∙ repl     : 替换匹配字符串的字符串\n",
    "∙ string  : 待匹配字符串\n",
    "∙ count   : 匹配的大替换次数 \n",
    "∙ flags   : 正则表达式使用时的控制标记\n",
    "\n",
    "等价用法：\n",
    "一次性：rst = re.search(r'[1-9]\\d{5}','bit 100081')\n",
    "多次性：pat = re.compile(r'[1-9]\\d{5}')\n",
    "        rst = pat.search('bit 100081')\n",
    "        \n",
    "Re库默认采用贪婪匹配，即输出匹配长的子串\n",
    "最小匹配：通过加？来实现\n",
    "*?     前一个字符0次或无限次扩展，最小匹配\n",
    "+?     前一个字符1次或无限次扩展，最小匹配\n",
    "??     前一个字符0次或1次扩展，最小匹配\n",
    "{m,n}? 扩展前一个字符m至n次（含n），最小匹配\n",
    "\n",
    "\n",
    "'''\n",
    "import re\n",
    "match1 = re.search(r'[1-9]\\d{5}','bit 100081')\n",
    "if match1:\n",
    "    print(match1.group(0))\n",
    "\n",
    "match2 = re.match(r'[1-9]\\d{5}','bit 100081')\n",
    "if match2:\n",
    "    print(match2.group(0))#为空\n",
    "\n",
    "ls = re.findall(r'[1-9]\\d{5}','bit100081 tsu100086')\n",
    "print(ls)\n",
    "\n",
    "re.split(r'[1-9]\\d{5}','bit100081 tsu100086')\n",
    "re.split(r'[1-9]\\d{5}','bit100081 tsu100086',maxsplit=1)\n",
    "\n",
    "for m in re.finditer(r'[1-9]\\d{5}','bit100081 tsu100086'):\n",
    "    if m:\n",
    "        print(m.group(0))\n",
    "\n",
    "re.sub(r'[1-9]\\d{5}',':zipcode','bit100081 tsu100086')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scrapy爬虫框架结构：‘5+2’\n",
    "1.spiders--engine--scheduler\n",
    "2.scheduler--engine--downloader--engine--spider\n",
    "3.spider--engine--item & scheduler\n",
    "\n",
    "Engine \n",
    "(1)控制所有模块之间的数据流 (2)根据条件触发事件\n",
    "Downloader \n",
    "根据请求下载网页\n",
    "Scheduler \n",
    "对所有爬取请求进行调度管理\n",
    "Downloader Middleware \n",
    "目的：实施Engine、Scheduler和Downloader 之间进行用户可配置的控制\n",
    "功能：修改、丢弃、新增请求或响应\n",
    "Spider \n",
    "(1)解析Downloader返回的响应（Response） \n",
    "(2)产生爬取项（scraped item） \n",
    "(3)产生额外的爬取请求（Request）\n",
    "Item Pipelines \n",
    "(1)以流水线方式处理Spider产生的爬取项\n",
    "(2)由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型 \n",
    "(3)可能操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库 \n",
    "Spider Middleware \n",
    "目的：对请求和爬取项的再处理 \n",
    "功能：修改、丢弃、新增请求或爬取项\n",
    "\n",
    "常用命令：（通过命令行实现）\n",
    ">scrapy<command> [options] [args]\n",
    "    *startproject创建一个新工程scrapystartproject<name>[dir]\n",
    "    *genspider创建一个爬虫scrapygenspider[options] <name> <domain> \n",
    "    settings获得爬虫配置信息scrapysettings [options] \n",
    "    *crawl运行一个爬虫scrapycrawl <spider> \n",
    "    list列出工程中所有爬虫scrapylist \n",
    "    shell启动URL调试命令行scrapyshell [url]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''demo.py文件源代码\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "class DemoSpider(scrapy.Spider):\n",
    "    name = \"demo\"\n",
    "    #allowed_domains = [\"python123.io\"]\n",
    "    **start_urls = ['https://python123.io/ws/demo.html']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        **fname = response.url.split('/')[-1]\n",
    "        **with open(fname, 'wb') as f:\n",
    "            **f.write(response.body)\n",
    "        **self.log('Saved file %s.' % name)\n",
    " \n",
    "start_urls = ['http://python123.io/ws/demo.heml']\n",
    "等价于\n",
    "def start_requests(self):\n",
    "    urls = [\n",
    "    'http://python123.io/ws/demo.heml'\n",
    "            ]\n",
    "    for url in urls:\n",
    "        yield scrapy.Request(url=url,callback=self.parse)\n",
    "步骤1：建立一个Scrapy爬虫工程\n",
    "1.d:  2.cd Python  3.scrapy startproject python123demo\n",
    "外层目录--python123demo\n",
    "部署scrapy爬虫的配置文件--scrapy.cfg\n",
    "Scrapy框架的用户自定义Python代码--python123demo/ \n",
    "初始化脚本--__init__.py \n",
    "Scrapy爬虫的配置文件--settings.py \n",
    "缓存目录，无需修改--__pycache__/\n",
    "\n",
    "步骤2：在工程中产生一个Scrapy爬虫 \n",
    "(1)生成一个名称为demo的spider \n",
    "(2)在spiders目录下增加代码文件demo.py\n",
    "scrapy genspider demo python123.io\n",
    "\n",
    "步骤3：配置产生的spider爬虫\n",
    "配置：（1）初始URL地址（2）获取页面后的解析方式\n",
    "\n",
    "步骤4：运行爬虫，获取网页 \n",
    "scrapy crawl demo\n",
    "\n",
    "scrapy爬虫的数据类型：\n",
    "\n",
    "request类：\n",
    "class scrapy.http.Request()\n",
    "Request对象表示一个HTTP请求，由Spider生成，由Downloader执行\n",
    ".urlRequest   对应的请求URL地址 \n",
    ".method       对应的请求方法，'GET''POST'等 \n",
    ".headers      字典类型风格的请求头 \n",
    ".body         请求内容主体，字符串类型 \n",
    ".meta         用户添加的扩展信息，在Scrapy内部模块间传递信息使用 \n",
    ".copy()       复制该请求\n",
    "\n",
    "response类：\n",
    "class scrapy.http.Response()\n",
    "Response对象表示一个HTTP响应，由Downloader生成，由Spider处理\n",
    ".urlResponse     对应的URL地址\n",
    ".statusHTTP      状态码，默认是200 \n",
    ".headersResponse 对应的头部信息 \n",
    ".bodyResponse    对应的内容信息，字符串类型 \n",
    ".flags           一组标记 \n",
    ".request         产生Response类型对应的Request对象 \n",
    ".copy()          复制该响应\n",
    "\n",
    "item类：\n",
    "class scrapy.item.Item()\n",
    "Item对象表示一个从HTML页面中提取的信息内容，由Spider生成，由Item Pipeline处理\n",
    "（Item类似字典类型，可以按照字典类型操作）\n",
    "\n",
    "CSS selector\n",
    "<HTML>.css('a::attr(href)').extract()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "包含yield语句的函数是一个生成器\n",
    "生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值\n",
    "生成器是一个不断产生值的函数\n",
    "优势:1)更节省存储空间 2)响应更迅速 3)使用更灵活\n",
    "\n",
    "\n",
    "'''\n",
    "def gen(n):\n",
    "    for i in range(n):\n",
    "        yield i**2\n",
    "for i in gen(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
